{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df041a4-cf0a-409b-9df4-365ff180ff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e96c809-1d70-41f9-bd07-a2e48aebe531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dde364d-1dfd-47dd-9b99-4b4366b808b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Could not load OpenAI model. Using default LlamaCPP=llama2-13b-chat. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
      "Original error:\n",
      "No API key found for OpenAI.\n",
      "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
      "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\n",
      "******\n",
      "Downloading url https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin to path /tmp/llama_index/models/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "total size (MB): 7323.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6985it [00:26, 264.96it/s]                          \n",
      "gguf_init_from_file: invalid magic number 67676a74\n",
      "error loading model: llama_model_loader: failed to load model from /tmp/llama_index/models/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nexus/posix0/MAGE-flaski/service/projects/data/Bioinformatics/bit_gpt_age/trained/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mload_index_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/indices/loading.py:33\u001b[0m, in \u001b[0;36mload_index_from_storage\u001b[0;34m(storage_context, index_id, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     index_ids \u001b[38;5;241m=\u001b[39m [index_id]\n\u001b[0;32m---> 33\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mload_indices_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/indices/loading.py:78\u001b[0m, in \u001b[0;36mload_indices_from_storage\u001b[0;34m(storage_context, index_ids, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m index_struct\u001b[38;5;241m.\u001b[39mget_type()\n\u001b[1;32m     77\u001b[0m     index_cls \u001b[38;5;241m=\u001b[39m INDEX_STRUCT_TYPE_TO_INDEX_CLASS[type_]\n\u001b[0;32m---> 78\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mindex_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     indices\u001b[38;5;241m.\u001b[39mappend(index)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:46\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/indices/base.py:61\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes must be a list of Node objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context \u001b[38;5;241m=\u001b[39m service_context \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context \u001b[38;5;241m=\u001b[39m storage_context \u001b[38;5;129;01mor\u001b[39;00m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mdocstore\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/indices/service_context.py:155\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both llm and llm_predictor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m     llm \u001b[38;5;241m=\u001b[39m resolve_llm(llm)\n\u001b[0;32m--> 155\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mLLMPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[1;32m    157\u001b[0m     llm_predictor\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:95\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, callback_manager, system_prompt, query_wrapper_prompt)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     llm: Optional[LLMType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     query_wrapper_prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback_manager:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/llms/utils.py:40\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_local \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm must start with str \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or of type LLM or BaseLanguageModel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         )\n\u001b[0;32m---> 40\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCPP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages_to_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages_to_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_to_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_to_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_gpu_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, BaseLanguageModel):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# NOTE: if it's a langchain model, wrap it in a LangChainLLM\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     llm \u001b[38;5;241m=\u001b[39m LangChainLLM(llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_index/llms/llama_cpp.py:107\u001b[0m, in \u001b[0;36mLlamaCPP.__init__\u001b[0;34m(self, model_url, model_path, temperature, max_new_tokens, context_window, messages_to_prompt, completion_to_prompt, callback_manager, generate_kwargs, model_kwargs, verbose)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_url(model_url, model_path)\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path)\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m model_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m    110\u001b[0m messages_to_prompt \u001b[38;5;241m=\u001b[39m messages_to_prompt \u001b[38;5;129;01mor\u001b[39;00m generic_messages_to_prompt\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/llama_cpp/llama.py:323\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr():\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m    322\u001b[0m         )\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_new_context_with_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"/nexus/posix0/MAGE-flaski/service/projects/data/Bioinformatics/bit_gpt_age/pmc_papers/\")\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa1744a-7e16-4554-806d-3f79df08f67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_index_from_storage in module llama_index.indices.loading:\n",
      "\n",
      "load_index_from_storage(storage_context: llama_index.storage.storage_context.StorageContext, index_id: Optional[str] = None, **kwargs: Any) -> llama_index.indices.base.BaseIndex\n",
      "    Load index from storage context.\n",
      "    \n",
      "    Args:\n",
      "        storage_context (StorageContext): storage context containing\n",
      "            docstore, index store and vector store.\n",
      "        index_id (Optional[str]): ID of the index to load.\n",
      "            Defaults to None, which assumes there's only a single index\n",
      "            in the index store and load it.\n",
      "        **kwargs: Additional keyword args to pass to the index constructors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_index_from_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fc0561-1207-4441-b3a4-adc870983827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VectorStoreIndex in module llama_index.indices.vector_store.base:\n",
      "\n",
      "class VectorStoreIndex(llama_index.indices.base.BaseIndex)\n",
      " |  VectorStoreIndex(nodes: Optional[Sequence[llama_index.schema.BaseNode]] = None, index_struct: Optional[llama_index.data_structs.data_structs.IndexDict] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, use_async: bool = False, store_nodes_override: bool = False, show_progress: bool = False, **kwargs: Any) -> None\n",
      " |  \n",
      " |  Vector Store Index.\n",
      " |  \n",
      " |  Args:\n",
      " |      use_async (bool): Whether to use asynchronous calls. Defaults to False.\n",
      " |      show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
      " |      store_nodes_override (bool): set to True to always store Node objects in index\n",
      " |          store and document store even if vector store keeps text. Defaults to False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VectorStoreIndex\n",
      " |      llama_index.indices.base.BaseIndex\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, nodes: Optional[Sequence[llama_index.schema.BaseNode]] = None, index_struct: Optional[llama_index.data_structs.data_structs.IndexDict] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, use_async: bool = False, store_nodes_override: bool = False, show_progress: bool = False, **kwargs: Any) -> None\n",
      " |      Initialize params.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: Any) -> llama_index.indices.base_retriever.BaseRetriever\n",
      " |  \n",
      " |  build_index_from_nodes(self, nodes: Sequence[llama_index.schema.BaseNode]) -> llama_index.data_structs.data_structs.IndexDict\n",
      " |      Build the index from nodes.\n",
      " |      \n",
      " |      NOTE: Overrides BaseIndex.build_index_from_nodes.\n",
      " |          VectorStoreIndex only stores nodes in document store\n",
      " |          if vector store does not store text\n",
      " |  \n",
      " |  delete_nodes(self, node_ids: List[str], delete_from_docstore: bool = False, **delete_kwargs: Any) -> None\n",
      " |      Delete a list of nodes from the index.\n",
      " |      \n",
      " |      Args:\n",
      " |          doc_ids (List[str]): A list of doc_ids from the nodes to delete\n",
      " |  \n",
      " |  delete_ref_doc(self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any) -> None\n",
      " |      Delete a document and it's nodes by using ref_doc_id.\n",
      " |  \n",
      " |  insert_nodes(self, nodes: Sequence[llama_index.schema.BaseNode], **insert_kwargs: Any) -> None\n",
      " |      Insert nodes.\n",
      " |      \n",
      " |      NOTE: overrides BaseIndex.insert_nodes.\n",
      " |          VectorStoreIndex only stores nodes in document store\n",
      " |          if vector store does not store text\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_vector_store(vector_store: llama_index.vector_stores.types.VectorStore, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, **kwargs: Any) -> 'VectorStoreIndex' from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  ref_doc_info\n",
      " |      Retrieve a dict mapping of ingested documents and their nodes+metadata.\n",
      " |  \n",
      " |  vector_store\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __orig_bases__ = (llama_index.indices.base.BaseIndex[llama_index.data_...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  index_struct_cls = <class 'llama_index.data_structs.data_structs.Index...\n",
      " |      A simple dictionary of documents.\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  as_chat_engine(self, chat_mode: llama_index.chat_engine.types.ChatMode = <ChatMode.BEST: 'best'>, **kwargs: Any) -> llama_index.chat_engine.types.BaseChatEngine\n",
      " |  \n",
      " |  as_query_engine(self, **kwargs: Any) -> llama_index.indices.query.base.BaseQueryEngine\n",
      " |  \n",
      " |  delete(self, doc_id: str, **delete_kwargs: Any) -> None\n",
      " |      Delete a document from the index.\n",
      " |      All nodes in the index related to the index will be deleted.\n",
      " |      \n",
      " |      Args:\n",
      " |          doc_id (str): A doc_id of the ingested document\n",
      " |  \n",
      " |  insert(self, document: llama_index.schema.Document, **insert_kwargs: Any) -> None\n",
      " |      Insert a document.\n",
      " |  \n",
      " |  refresh(self, documents: Sequence[llama_index.schema.Document], **update_kwargs: Any) -> List[bool]\n",
      " |      Refresh an index with documents that have changed.\n",
      " |      \n",
      " |      This allows users to save LLM and Embedding model calls, while only\n",
      " |      updating documents that have any changes in text or metadata. It\n",
      " |      will also insert any documents that previously were not stored.\n",
      " |  \n",
      " |  refresh_ref_docs(self, documents: Sequence[llama_index.schema.Document], **update_kwargs: Any) -> List[bool]\n",
      " |      Refresh an index with documents that have changed.\n",
      " |      \n",
      " |      This allows users to save LLM and Embedding model calls, while only\n",
      " |      updating documents that have any changes in text or metadata. It\n",
      " |      will also insert any documents that previously were not stored.\n",
      " |  \n",
      " |  set_index_id(self, index_id: str) -> None\n",
      " |      Set the index id.\n",
      " |      \n",
      " |      NOTE: if you decide to set the index_id on the index_struct manually,\n",
      " |      you will need to explicitly call `add_index_struct` on the `index_store`\n",
      " |      to update the index store.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |          index.index_struct.index_id = index_id\n",
      " |          index.storage_context.index_store.add_index_struct(index.index_struct)\n",
      " |      \n",
      " |      Args:\n",
      " |          index_id (str): Index id to set.\n",
      " |  \n",
      " |  update(self, document: llama_index.schema.Document, **update_kwargs: Any) -> None\n",
      " |      Update a document and it's corresponding nodes.\n",
      " |      \n",
      " |      This is equivalent to deleting the document and then inserting it again.\n",
      " |      \n",
      " |      Args:\n",
      " |          document (Union[BaseDocument, BaseIndex]): document to update\n",
      " |          insert_kwargs (Dict): kwargs to pass to insert\n",
      " |          delete_kwargs (Dict): kwargs to pass to delete\n",
      " |  \n",
      " |  update_ref_doc(self, document: llama_index.schema.Document, **update_kwargs: Any) -> None\n",
      " |      Update a document and it's corresponding nodes.\n",
      " |      \n",
      " |      This is equivalent to deleting the document and then inserting it again.\n",
      " |      \n",
      " |      Args:\n",
      " |          document (Union[BaseDocument, BaseIndex]): document to update\n",
      " |          insert_kwargs (Dict): kwargs to pass to insert\n",
      " |          delete_kwargs (Dict): kwargs to pass to delete\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  from_documents(documents: Sequence[llama_index.schema.Document], storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, show_progress: bool = False, **kwargs: Any) -> ~IndexType from abc.ABCMeta\n",
      " |      Create index from documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (Optional[Sequence[BaseDocument]]): List of documents to\n",
      " |              build the index from.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  docstore\n",
      " |      Get the docstore corresponding to the index.\n",
      " |  \n",
      " |  index_id\n",
      " |      Get the index struct.\n",
      " |  \n",
      " |  index_struct\n",
      " |      Get the index struct.\n",
      " |  \n",
      " |  service_context\n",
      " |  \n",
      " |  storage_context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  summary\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c637fe-9f9a-4f79-9305-a520264e2040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_defaults in module llama_index.storage.storage_context:\n",
      "\n",
      "from_defaults(docstore: Optional[llama_index.storage.docstore.types.BaseDocumentStore] = None, index_store: Optional[llama_index.storage.index_store.types.BaseIndexStore] = None, vector_store: Optional[llama_index.vector_stores.types.VectorStore] = None, graph_store: Optional[llama_index.graph_stores.types.GraphStore] = None, persist_dir: Optional[str] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> 'StorageContext' method of builtins.type instance\n",
      "    Create a StorageContext from defaults.\n",
      "    \n",
      "    Args:\n",
      "        docstore (Optional[BaseDocumentStore]): document store\n",
      "        index_store (Optional[BaseIndexStore]): index store\n",
      "        vector_store (Optional[VectorStore]): vector store\n",
      "        graph_store (Optional[GraphStore]): graph store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StorageContext.from_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ea9923-ba39-4f8b-b79b-2892a7a7f69e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StorageContext in module llama_index.storage.storage_context:\n",
      "\n",
      "class StorageContext(builtins.object)\n",
      " |  StorageContext(docstore: llama_index.storage.docstore.types.BaseDocumentStore, index_store: llama_index.storage.index_store.types.BaseIndexStore, vector_store: llama_index.vector_stores.types.VectorStore, graph_store: llama_index.graph_stores.types.GraphStore) -> None\n",
      " |  \n",
      " |  Storage context.\n",
      " |  \n",
      " |  The storage context container is a utility container for storing nodes,\n",
      " |  indices, and vectors. It contains the following:\n",
      " |  - docstore: BaseDocumentStore\n",
      " |  - index_store: BaseIndexStore\n",
      " |  - vector_store: VectorStore\n",
      " |  - graph_store: GraphStore\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, docstore: llama_index.storage.docstore.types.BaseDocumentStore, index_store: llama_index.storage.index_store.types.BaseIndexStore, vector_store: llama_index.vector_stores.types.VectorStore, graph_store: llama_index.graph_stores.types.GraphStore) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  persist(self, persist_dir: str = './storage', docstore_fname: str = 'docstore.json', index_store_fname: str = 'index_store.json', vector_store_fname: str = 'vector_store.json', graph_store_fname: str = 'graph_store.json', fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> None\n",
      " |      Persist the storage context.\n",
      " |      \n",
      " |      Args:\n",
      " |          persist_dir (str): directory to persist the storage context\n",
      " |  \n",
      " |  to_dict(self) -> dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_defaults(docstore: Optional[llama_index.storage.docstore.types.BaseDocumentStore] = None, index_store: Optional[llama_index.storage.index_store.types.BaseIndexStore] = None, vector_store: Optional[llama_index.vector_stores.types.VectorStore] = None, graph_store: Optional[llama_index.graph_stores.types.GraphStore] = None, persist_dir: Optional[str] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> 'StorageContext' from builtins.type\n",
      " |      Create a StorageContext from defaults.\n",
      " |      \n",
      " |      Args:\n",
      " |          docstore (Optional[BaseDocumentStore]): document store\n",
      " |          index_store (Optional[BaseIndexStore]): index store\n",
      " |          vector_store (Optional[VectorStore]): vector store\n",
      " |          graph_store (Optional[GraphStore]): graph store\n",
      " |  \n",
      " |  from_dict(save_dict: dict) -> 'StorageContext' from builtins.type\n",
      " |      Create a StorageContext from dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'docstore': <class 'llama_index.storage.docstore.ty...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'docstore': Field(name='docstore',type=<class ...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('docstore', 'index_store', 'vector_store', 'graph_st...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StorageContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff153ba-2c5d-4cba-9dc2-6908992198b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb51207-76eb-4a96-9528-c555829b8757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_documents in module llama_index.indices.base:\n",
      "\n",
      "from_documents(documents: Sequence[llama_index.schema.Document], storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, show_progress: bool = False, **kwargs: Any) -> ~IndexType method of abc.ABCMeta instance\n",
      "    Create index from documents.\n",
      "    \n",
      "    Args:\n",
      "        documents (Optional[Sequence[BaseDocument]]): List of documents to\n",
      "            build the index from.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VectorStoreIndex.from_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d312ddfc-8653-4a85-b305-3dbe4da3b6da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StorageContext in module llama_index.storage.storage_context:\n",
      "\n",
      "class StorageContext(builtins.object)\n",
      " |  StorageContext(docstore: llama_index.storage.docstore.types.BaseDocumentStore, index_store: llama_index.storage.index_store.types.BaseIndexStore, vector_store: llama_index.vector_stores.types.VectorStore, graph_store: llama_index.graph_stores.types.GraphStore) -> None\n",
      " |  \n",
      " |  Storage context.\n",
      " |  \n",
      " |  The storage context container is a utility container for storing nodes,\n",
      " |  indices, and vectors. It contains the following:\n",
      " |  - docstore: BaseDocumentStore\n",
      " |  - index_store: BaseIndexStore\n",
      " |  - vector_store: VectorStore\n",
      " |  - graph_store: GraphStore\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, docstore: llama_index.storage.docstore.types.BaseDocumentStore, index_store: llama_index.storage.index_store.types.BaseIndexStore, vector_store: llama_index.vector_stores.types.VectorStore, graph_store: llama_index.graph_stores.types.GraphStore) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  persist(self, persist_dir: str = './storage', docstore_fname: str = 'docstore.json', index_store_fname: str = 'index_store.json', vector_store_fname: str = 'vector_store.json', graph_store_fname: str = 'graph_store.json', fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> None\n",
      " |      Persist the storage context.\n",
      " |      \n",
      " |      Args:\n",
      " |          persist_dir (str): directory to persist the storage context\n",
      " |  \n",
      " |  to_dict(self) -> dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_defaults(docstore: Optional[llama_index.storage.docstore.types.BaseDocumentStore] = None, index_store: Optional[llama_index.storage.index_store.types.BaseIndexStore] = None, vector_store: Optional[llama_index.vector_stores.types.VectorStore] = None, graph_store: Optional[llama_index.graph_stores.types.GraphStore] = None, persist_dir: Optional[str] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> 'StorageContext' from builtins.type\n",
      " |      Create a StorageContext from defaults.\n",
      " |      \n",
      " |      Args:\n",
      " |          docstore (Optional[BaseDocumentStore]): document store\n",
      " |          index_store (Optional[BaseIndexStore]): index store\n",
      " |          vector_store (Optional[VectorStore]): vector store\n",
      " |          graph_store (Optional[GraphStore]): graph store\n",
      " |  \n",
      " |  from_dict(save_dict: dict) -> 'StorageContext' from builtins.type\n",
      " |      Create a StorageContext from dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'docstore': <class 'llama_index.storage.docstore.ty...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'docstore': Field(name='docstore',type=<class ...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('docstore', 'index_store', 'vector_store', 'graph_st...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "help( llama_index.storage.storage_context.StorageContext )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f7d371-66d5-4898-a704-4ec9c25c2502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ServiceContext in module llama_index.indices.service_context:\n",
      "\n",
      "class ServiceContext(builtins.object)\n",
      " |  ServiceContext(llm_predictor: llama_index.llm_predictor.base.BaseLLMPredictor, prompt_helper: llama_index.indices.prompt_helper.PromptHelper, embed_model: llama_index.embeddings.base.BaseEmbedding, node_parser: llama_index.node_parser.interface.NodeParser, llama_logger: llama_index.logger.base.LlamaLogger, callback_manager: llama_index.callbacks.base.CallbackManager) -> None\n",
      " |  \n",
      " |  Service Context container.\n",
      " |  \n",
      " |  The service context container is a utility container for LlamaIndex\n",
      " |  index and query classes. It contains the following:\n",
      " |  - llm_predictor: BaseLLMPredictor\n",
      " |  - prompt_helper: PromptHelper\n",
      " |  - embed_model: BaseEmbedding\n",
      " |  - node_parser: NodeParser\n",
      " |  - llama_logger: LlamaLogger (deprecated)\n",
      " |  - callback_manager: CallbackManager\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, llm_predictor: llama_index.llm_predictor.base.BaseLLMPredictor, prompt_helper: llama_index.indices.prompt_helper.PromptHelper, embed_model: llama_index.embeddings.base.BaseEmbedding, node_parser: llama_index.node_parser.interface.NodeParser, llama_logger: llama_index.logger.base.LlamaLogger, callback_manager: llama_index.callbacks.base.CallbackManager) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __pydantic_validate_values__ = _dataclass_validate_values(self: 'Dataclass') -> None\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  to_dict(self) -> dict\n",
      " |      Convert service context to dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  __get_validators__ = _get_validators() -> 'CallableGenerator' from builtins.type\n",
      " |  \n",
      " |  __validate__ = _validate_dataclass(v: Any) -> 'DataclassT' from builtins.type\n",
      " |  \n",
      " |  from_defaults(llm_predictor: Optional[llama_index.llm_predictor.base.BaseLLMPredictor] = None, llm: Union[str, llama_index.llms.base.LLM, langchain.schema.language_model.BaseLanguageModel, NoneType] = 'default', prompt_helper: Optional[llama_index.indices.prompt_helper.PromptHelper] = None, embed_model: Union[llama_index.embeddings.base.BaseEmbedding, langchain.embeddings.base.Embeddings, str, NoneType] = 'default', node_parser: Optional[llama_index.node_parser.interface.NodeParser] = None, llama_logger: Optional[llama_index.logger.base.LlamaLogger] = None, callback_manager: Optional[llama_index.callbacks.base.CallbackManager] = None, system_prompt: Optional[str] = None, query_wrapper_prompt: Optional[llama_index.prompts.base.BasePromptTemplate] = None, chunk_size: Optional[int] = None, chunk_overlap: Optional[int] = None, context_window: Optional[int] = None, num_output: Optional[int] = None, chunk_size_limit: Optional[int] = None) -> 'ServiceContext' from builtins.type\n",
      " |      Create a ServiceContext from defaults.\n",
      " |      If an argument is specified, then use the argument value provided for that\n",
      " |      parameter. If an argument is not specified, then use the default value.\n",
      " |      \n",
      " |      You can change the base defaults by setting llama_index.global_service_context\n",
      " |      to a ServiceContext object with your desired settings.\n",
      " |      \n",
      " |      Args:\n",
      " |          llm_predictor (Optional[BaseLLMPredictor]): LLMPredictor\n",
      " |          prompt_helper (Optional[PromptHelper]): PromptHelper\n",
      " |          embed_model (Optional[BaseEmbedding]): BaseEmbedding\n",
      " |              or \"local\" (use local model)\n",
      " |          node_parser (Optional[NodeParser]): NodeParser\n",
      " |          llama_logger (Optional[LlamaLogger]): LlamaLogger (deprecated)\n",
      " |          chunk_size (Optional[int]): chunk_size\n",
      " |          callback_manager (Optional[CallbackManager]): CallbackManager\n",
      " |          system_prompt (Optional[str]): System-wide prompt to be prepended\n",
      " |              to all input prompts, used to guide system \"decision making\"\n",
      " |          query_wrapper_prompt (Optional[BasePromptTemplate]): A format to wrap\n",
      " |              passed-in input queries.\n",
      " |      \n",
      " |      Deprecated Args:\n",
      " |          chunk_size_limit (Optional[int]): renamed to chunk_size\n",
      " |  \n",
      " |  from_dict(data: dict) -> 'ServiceContext' from builtins.type\n",
      " |  \n",
      " |  from_service_context(service_context: 'ServiceContext', llm_predictor: Optional[llama_index.llm_predictor.base.BaseLLMPredictor] = None, llm: Union[str, llama_index.llms.base.LLM, langchain.schema.language_model.BaseLanguageModel, NoneType] = 'default', prompt_helper: Optional[llama_index.indices.prompt_helper.PromptHelper] = None, embed_model: Union[llama_index.embeddings.base.BaseEmbedding, langchain.embeddings.base.Embeddings, str, NoneType] = 'default', node_parser: Optional[llama_index.node_parser.interface.NodeParser] = None, llama_logger: Optional[llama_index.logger.base.LlamaLogger] = None, callback_manager: Optional[llama_index.callbacks.base.CallbackManager] = None, system_prompt: Optional[str] = None, query_wrapper_prompt: Optional[llama_index.prompts.base.BasePromptTemplate] = None, chunk_size: Optional[int] = None, chunk_overlap: Optional[int] = None, context_window: Optional[int] = None, num_output: Optional[int] = None, chunk_size_limit: Optional[int] = None) -> 'ServiceContext' from builtins.type\n",
      " |      Instantiate a new service context using a previous as the defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  llm\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'callback_manager': <class 'llama_index.callbacks.b...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'callback_manager': Field(name='callback_manag...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('llm_predictor', 'prompt_helper', 'embed_model', 'no...\n",
      " |  \n",
      " |  __pydantic_initialised__ = False\n",
      " |  \n",
      " |  __pydantic_model__ = <class 'llama_index.indices.service_context.Servi...\n",
      " |  \n",
      " |  __pydantic_run_validation__ = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(llama_index.indices.service_context.ServiceContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f5bbaf0-4fc3-4e17-b19c-d714870ac9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /nexus/posix0/MAGE-flaski/service/posit/home/jboucas/.cache/torch/sentence_transformers/TheBloke_Llama-2-13B-chat-GGML. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /nexus/posix0/MAGE-flaski/service/posit/home/jboucas/.cache/torch/sentence_transformers/TheBloke_Llama-2-13B-chat-GGML.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m LangchainEmbedding(\u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTheBloke/Llama-2-13B-chat-GGML\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/langchain/embeddings/huggingface.py:64\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence_transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:97\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     95\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(model_path)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    100\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:806\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03mCreates a simple Transformer + Mean Pooling model and returns the modules\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    805\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence-transformers model found with name \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Creating a new one with MEAN pooling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name_or_path))\n\u001b[0;32m--> 806\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [transformer_model, pooling_model]\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:2650\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2646\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2647\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2648\u001b[0m         )\n\u001b[1;32m   2649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2651\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2652\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2653\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2654\u001b[0m         )\n\u001b[1;32m   2655\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   2656\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /nexus/posix0/MAGE-flaski/service/posit/home/jboucas/.cache/torch/sentence_transformers/TheBloke_Llama-2-13B-chat-GGML."
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name='TheBloke/Llama-2-13B-chat-GGML'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0d0489-2cff-4453-9c60-6d2306c50877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cc509f884c4240accc85e385798f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ab102/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf5857570d848ee8ec2b265146fc695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce99f1258b246639def7d8ba7d7f384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2d2d7ab102/README.md:   0%|          | 0.00/78.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d23ceead464fae9672bb528c643573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2d7ab102/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bafd12ef5294d2fb43cc3ffbf88cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1642514afc94509aa0c302396f3b2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f39a587f4744c93add2350ba87edc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a1397c8f8c4e65affaff47498eae38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b3cf262b9a443a803c3f2b8d44c9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0977775cd9da43f8867da12e26168c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ab102/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52623782753a41369589dacc3cbc20ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd852ae58aa046ffb7e4200b43f09454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2d2d7ab102/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb830705a074c10938cb2585ee4f2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d7ab102/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import LLMPredictor, ServiceContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name='BAAI/bge-small-en'))\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "documents = SimpleDirectoryReader('/flaski_private/pmc_papers', recursive=True ).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10cac04-da0d-4011-9bb2-98dd02108298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StorageContext in module llama_index.storage.storage_context:\n",
      "\n",
      "class StorageContext(builtins.object)\n",
      " |  StorageContext(docstore: llama_index.storage.docstore.types.BaseDocumentStore, index_store: llama_index.storage.index_store.types.BaseIndexStore, vector_store: llama_index.vector_stores.types.VectorStore, graph_store: llama_index.graph_stores.types.GraphStore) -> None\n",
      " |  \n",
      " |  Storage context.\n",
      " |  \n",
      " |  The storage context container is a utility container for storing nodes,\n",
      " |  indices, and vectors. It contains the following:\n",
      " |  - docstore: BaseDocumentStore\n",
      " |  - index_store: BaseIndexStore\n",
      " |  - vector_store: VectorStore\n",
      " |  - graph_store: GraphStore\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, docstore: llama_index.storage.docstore.types.BaseDocumentStore, index_store: llama_index.storage.index_store.types.BaseIndexStore, vector_store: llama_index.vector_stores.types.VectorStore, graph_store: llama_index.graph_stores.types.GraphStore) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  persist(self, persist_dir: str = './storage', docstore_fname: str = 'docstore.json', index_store_fname: str = 'index_store.json', vector_store_fname: str = 'vector_store.json', graph_store_fname: str = 'graph_store.json', fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> None\n",
      " |      Persist the storage context.\n",
      " |      \n",
      " |      Args:\n",
      " |          persist_dir (str): directory to persist the storage context\n",
      " |  \n",
      " |  to_dict(self) -> dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_defaults(docstore: Optional[llama_index.storage.docstore.types.BaseDocumentStore] = None, index_store: Optional[llama_index.storage.index_store.types.BaseIndexStore] = None, vector_store: Optional[llama_index.vector_stores.types.VectorStore] = None, graph_store: Optional[llama_index.graph_stores.types.GraphStore] = None, persist_dir: Optional[str] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> 'StorageContext' from builtins.type\n",
      " |      Create a StorageContext from defaults.\n",
      " |      \n",
      " |      Args:\n",
      " |          docstore (Optional[BaseDocumentStore]): document store\n",
      " |          index_store (Optional[BaseIndexStore]): index store\n",
      " |          vector_store (Optional[VectorStore]): vector store\n",
      " |          graph_store (Optional[GraphStore]): graph store\n",
      " |  \n",
      " |  from_dict(save_dict: dict) -> 'StorageContext' from builtins.type\n",
      " |      Create a StorageContext from dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'docstore': <class 'llama_index.storage.docstore.ty...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'docstore': Field(name='docstore',type=<class ...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('docstore', 'index_store', 'vector_store', 'graph_st...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StorageContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16aa3366-258a-4118-80e7-1a46dd53478c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_defaults in module llama_index.indices.service_context:\n",
      "\n",
      "from_defaults(llm_predictor: Optional[llama_index.llm_predictor.base.BaseLLMPredictor] = None, llm: Union[str, llama_index.llms.base.LLM, langchain.schema.language_model.BaseLanguageModel, NoneType] = 'default', prompt_helper: Optional[llama_index.indices.prompt_helper.PromptHelper] = None, embed_model: Union[llama_index.embeddings.base.BaseEmbedding, langchain.embeddings.base.Embeddings, str, NoneType] = 'default', node_parser: Optional[llama_index.node_parser.interface.NodeParser] = None, llama_logger: Optional[llama_index.logger.base.LlamaLogger] = None, callback_manager: Optional[llama_index.callbacks.base.CallbackManager] = None, system_prompt: Optional[str] = None, query_wrapper_prompt: Optional[llama_index.prompts.base.BasePromptTemplate] = None, chunk_size: Optional[int] = None, chunk_overlap: Optional[int] = None, context_window: Optional[int] = None, num_output: Optional[int] = None, chunk_size_limit: Optional[int] = None) -> 'ServiceContext' method of builtins.type instance\n",
      "    Create a ServiceContext from defaults.\n",
      "    If an argument is specified, then use the argument value provided for that\n",
      "    parameter. If an argument is not specified, then use the default value.\n",
      "    \n",
      "    You can change the base defaults by setting llama_index.global_service_context\n",
      "    to a ServiceContext object with your desired settings.\n",
      "    \n",
      "    Args:\n",
      "        llm_predictor (Optional[BaseLLMPredictor]): LLMPredictor\n",
      "        prompt_helper (Optional[PromptHelper]): PromptHelper\n",
      "        embed_model (Optional[BaseEmbedding]): BaseEmbedding\n",
      "            or \"local\" (use local model)\n",
      "        node_parser (Optional[NodeParser]): NodeParser\n",
      "        llama_logger (Optional[LlamaLogger]): LlamaLogger (deprecated)\n",
      "        chunk_size (Optional[int]): chunk_size\n",
      "        callback_manager (Optional[CallbackManager]): CallbackManager\n",
      "        system_prompt (Optional[str]): System-wide prompt to be prepended\n",
      "            to all input prompts, used to guide system \"decision making\"\n",
      "        query_wrapper_prompt (Optional[BasePromptTemplate]): A format to wrap\n",
      "            passed-in input queries.\n",
      "    \n",
      "    Deprecated Args:\n",
      "        chunk_size_limit (Optional[int]): renamed to chunk_size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ServiceContext.from_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e91a56dd-3c28-44d6-8f7a-44c66f5f9337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m LLMPredictor(llm\u001b[38;5;241m=\u001b[39m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama2-13b-chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.jupyter/python/3.10/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"llama2-13b-chat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a0af4f-fb4c-421b-88b3-5418b5b249af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LangchainEmbedding in module llama_index.embeddings.langchain:\n",
      "\n",
      "class LangchainEmbedding(llama_index.embeddings.base.BaseEmbedding)\n",
      " |  LangchainEmbedding(langchain_embeddings: langchain.embeddings.base.Embeddings, model_name: Optional[str] = None, embed_batch_size: int = 10, callback_manager: Optional[llama_index.callbacks.base.CallbackManager] = None) -> None\n",
      " |  \n",
      " |  External embeddings (taken from Langchain).\n",
      " |  \n",
      " |  Args:\n",
      " |      langchain_embedding (langchain.embeddings.Embeddings): Langchain\n",
      " |          embeddings class.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LangchainEmbedding\n",
      " |      llama_index.embeddings.base.BaseEmbedding\n",
      " |      llama_index.schema.BaseComponent\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, langchain_embeddings: langchain.embeddings.base.Embeddings, model_name: Optional[str] = None, embed_batch_size: int = 10, callback_manager: Optional[llama_index.callbacks.base.CallbackManager] = None)\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  class_name() -> str from pydantic.v1.main.ModelMetaclass\n",
      " |      Get class name.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_langchain_embedding': <class 'langchain.embedding...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True}\n",
      " |  \n",
      " |  __fields__ = {'callback_manager': ModelField(name='callback_manager', ...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {'_langchain_embedding': ModelPrivateAttr(def...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (langchain_embeddings: langchain.embe....ca...\n",
      " |  \n",
      " |  __validators__ = {'callback_manager': [<pydantic.v1.class_validators.V...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.embeddings.base.BaseEmbedding:\n",
      " |  \n",
      " |  async aget_agg_embedding_from_queries(self, queries: List[str], agg_fn: Optional[Callable[..., List[float]]] = None) -> List[float]\n",
      " |      Get aggregated embedding from multiple queries.\n",
      " |  \n",
      " |  async aget_query_embedding(self, query: str) -> List[float]\n",
      " |      Get query embedding.\n",
      " |  \n",
      " |  async aget_queued_text_embeddings(self, text_queue: List[Tuple[str, str]], show_progress: bool = False) -> Tuple[List[str], List[List[float]]]\n",
      " |      Asynchronously get a list of text embeddings.\n",
      " |      \n",
      " |      Call async embedding API to get embeddings for all queued texts in parallel.\n",
      " |      Argument `text_queue` must be passed in to avoid updating it async.\n",
      " |  \n",
      " |  get_agg_embedding_from_queries(self, queries: List[str], agg_fn: Optional[Callable[..., List[float]]] = None) -> List[float]\n",
      " |      Get aggregated embedding from multiple queries.\n",
      " |  \n",
      " |  get_query_embedding(self, query: str) -> List[float]\n",
      " |      Get query embedding.\n",
      " |  \n",
      " |  get_queued_text_embeddings(self, show_progress: bool = False) -> Tuple[List[str], List[List[float]]]\n",
      " |      Get queued text embeddings.\n",
      " |      \n",
      " |      Call embedding API to get embeddings for all queued texts.\n",
      " |  \n",
      " |  get_text_embedding(self, text: str) -> List[float]\n",
      " |      Get text embedding.\n",
      " |  \n",
      " |  queue_text_for_embedding(self, text_id: str, text: str) -> None\n",
      " |      Queue text for embedding.\n",
      " |      \n",
      " |      Used for batching texts during embedding calls.\n",
      " |  \n",
      " |  similarity(self, embedding1: List, embedding2: List, mode: llama_index.embeddings.base.SimilarityMode = <SimilarityMode.DEFAULT: 'cosine'>) -> float\n",
      " |      Get embedding similarity.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from llama_index.embeddings.base.BaseEmbedding:\n",
      " |  \n",
      " |  Config = <class 'llama_index.embeddings.base.BaseEmbedding.Config'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.schema.BaseComponent:\n",
      " |  \n",
      " |  to_dict(self, **kwargs: Any) -> Dict[str, Any]\n",
      " |  \n",
      " |  to_json(self, **kwargs: Any) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.schema.BaseComponent:\n",
      " |  \n",
      " |  from_dict(data: Dict[str, Any], **kwargs: Any) -> typing_extensions.Self from pydantic.v1.main.ModelMetaclass\n",
      " |      # TODO: return type here not supported by current mypy version\n",
      " |  \n",
      " |  from_json(data_str: str, **kwargs: Any) -> typing_extensions.Self from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LangchainEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a54c7cb0-ab05-4438-9401-7c810b12e38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HuggingFaceEmbeddings in module langchain.embeddings.huggingface:\n",
      "\n",
      "class HuggingFaceEmbeddings(pydantic.v1.main.BaseModel, langchain.embeddings.base.Embeddings)\n",
      " |  HuggingFaceEmbeddings(*, client: Any = None, model_name: str = 'sentence-transformers/all-mpnet-base-v2', cache_folder: Optional[str] = None, model_kwargs: Dict[str, Any] = None, encode_kwargs: Dict[str, Any] = None, multi_process: bool = False) -> None\n",
      " |  \n",
      " |  HuggingFace sentence_transformers embedding models.\n",
      " |  \n",
      " |  To use, you should have the ``sentence_transformers`` python package installed.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain.embeddings import HuggingFaceEmbeddings\n",
      " |  \n",
      " |          model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
      " |          model_kwargs = {'device': 'cpu'}\n",
      " |          encode_kwargs = {'normalize_embeddings': False}\n",
      " |          hf = HuggingFaceEmbeddings(\n",
      " |              model_name=model_name,\n",
      " |              model_kwargs=model_kwargs,\n",
      " |              encode_kwargs=encode_kwargs\n",
      " |          )\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HuggingFaceEmbeddings\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain.embeddings.base.Embeddings\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any)\n",
      " |      Initialize the sentence_transformer.\n",
      " |  \n",
      " |  embed_documents(self, texts: List[str]) -> List[List[float]]\n",
      " |      Compute doc embeddings using a HuggingFace transformer model.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts: The list of texts to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of embeddings, one for each text.\n",
      " |  \n",
      " |  embed_query(self, text: str) -> List[float]\n",
      " |      Compute query embeddings using a HuggingFace transformer model.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The text to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Embeddings for the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain.embeddings.huggingface.HuggingFaceEmbedding...\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'cache_folder': typing.Optional[str], 'client': typ...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = None\n",
      " |  \n",
      " |  __fields__ = {'cache_folder': ModelField(name='cache_folder', type=Opt...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, client: Any = None, model_name: s...ny]...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.embeddings.base.Embeddings:\n",
      " |  \n",
      " |  async aembed_documents(self, texts: List[str]) -> List[List[float]]\n",
      " |      Asynchronous Embed search docs.\n",
      " |  \n",
      " |  async aembed_query(self, text: str) -> List[float]\n",
      " |      Asynchronous Embed query text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HuggingFaceEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f38659-23bc-4e74-bd98-2ff0f8b86988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_documents in module llama_index.indices.base:\n",
      "\n",
      "from_documents(documents: Sequence[llama_index.schema.Document], storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, show_progress: bool = False, **kwargs: Any) -> ~IndexType method of abc.ABCMeta instance\n",
      "    Create index from documents.\n",
      "    \n",
      "    Args:\n",
      "        documents (Optional[Sequence[BaseDocument]]): List of documents to\n",
      "            build the index from.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VectorStoreIndex.from_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c7b6079-5bab-4aab-8323-05ec812ff934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VectorStoreIndex in module llama_index.indices.vector_store.base:\n",
      "\n",
      "class VectorStoreIndex(llama_index.indices.base.BaseIndex)\n",
      " |  VectorStoreIndex(nodes: Optional[Sequence[llama_index.schema.BaseNode]] = None, index_struct: Optional[llama_index.data_structs.data_structs.IndexDict] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, use_async: bool = False, store_nodes_override: bool = False, show_progress: bool = False, **kwargs: Any) -> None\n",
      " |  \n",
      " |  Vector Store Index.\n",
      " |  \n",
      " |  Args:\n",
      " |      use_async (bool): Whether to use asynchronous calls. Defaults to False.\n",
      " |      show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
      " |      store_nodes_override (bool): set to True to always store Node objects in index\n",
      " |          store and document store even if vector store keeps text. Defaults to False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VectorStoreIndex\n",
      " |      llama_index.indices.base.BaseIndex\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, nodes: Optional[Sequence[llama_index.schema.BaseNode]] = None, index_struct: Optional[llama_index.data_structs.data_structs.IndexDict] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, use_async: bool = False, store_nodes_override: bool = False, show_progress: bool = False, **kwargs: Any) -> None\n",
      " |      Initialize params.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: Any) -> llama_index.indices.base_retriever.BaseRetriever\n",
      " |  \n",
      " |  build_index_from_nodes(self, nodes: Sequence[llama_index.schema.BaseNode]) -> llama_index.data_structs.data_structs.IndexDict\n",
      " |      Build the index from nodes.\n",
      " |      \n",
      " |      NOTE: Overrides BaseIndex.build_index_from_nodes.\n",
      " |          VectorStoreIndex only stores nodes in document store\n",
      " |          if vector store does not store text\n",
      " |  \n",
      " |  delete_nodes(self, node_ids: List[str], delete_from_docstore: bool = False, **delete_kwargs: Any) -> None\n",
      " |      Delete a list of nodes from the index.\n",
      " |      \n",
      " |      Args:\n",
      " |          doc_ids (List[str]): A list of doc_ids from the nodes to delete\n",
      " |  \n",
      " |  delete_ref_doc(self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any) -> None\n",
      " |      Delete a document and it's nodes by using ref_doc_id.\n",
      " |  \n",
      " |  insert_nodes(self, nodes: Sequence[llama_index.schema.BaseNode], **insert_kwargs: Any) -> None\n",
      " |      Insert nodes.\n",
      " |      \n",
      " |      NOTE: overrides BaseIndex.insert_nodes.\n",
      " |          VectorStoreIndex only stores nodes in document store\n",
      " |          if vector store does not store text\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_vector_store(vector_store: llama_index.vector_stores.types.VectorStore, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, **kwargs: Any) -> 'VectorStoreIndex' from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  ref_doc_info\n",
      " |      Retrieve a dict mapping of ingested documents and their nodes+metadata.\n",
      " |  \n",
      " |  vector_store\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __orig_bases__ = (llama_index.indices.base.BaseIndex[llama_index.data_...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  index_struct_cls = <class 'llama_index.data_structs.data_structs.Index...\n",
      " |      A simple dictionary of documents.\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  as_chat_engine(self, chat_mode: llama_index.chat_engine.types.ChatMode = <ChatMode.BEST: 'best'>, **kwargs: Any) -> llama_index.chat_engine.types.BaseChatEngine\n",
      " |  \n",
      " |  as_query_engine(self, **kwargs: Any) -> llama_index.indices.query.base.BaseQueryEngine\n",
      " |  \n",
      " |  delete(self, doc_id: str, **delete_kwargs: Any) -> None\n",
      " |      Delete a document from the index.\n",
      " |      All nodes in the index related to the index will be deleted.\n",
      " |      \n",
      " |      Args:\n",
      " |          doc_id (str): A doc_id of the ingested document\n",
      " |  \n",
      " |  insert(self, document: llama_index.schema.Document, **insert_kwargs: Any) -> None\n",
      " |      Insert a document.\n",
      " |  \n",
      " |  refresh(self, documents: Sequence[llama_index.schema.Document], **update_kwargs: Any) -> List[bool]\n",
      " |      Refresh an index with documents that have changed.\n",
      " |      \n",
      " |      This allows users to save LLM and Embedding model calls, while only\n",
      " |      updating documents that have any changes in text or metadata. It\n",
      " |      will also insert any documents that previously were not stored.\n",
      " |  \n",
      " |  refresh_ref_docs(self, documents: Sequence[llama_index.schema.Document], **update_kwargs: Any) -> List[bool]\n",
      " |      Refresh an index with documents that have changed.\n",
      " |      \n",
      " |      This allows users to save LLM and Embedding model calls, while only\n",
      " |      updating documents that have any changes in text or metadata. It\n",
      " |      will also insert any documents that previously were not stored.\n",
      " |  \n",
      " |  set_index_id(self, index_id: str) -> None\n",
      " |      Set the index id.\n",
      " |      \n",
      " |      NOTE: if you decide to set the index_id on the index_struct manually,\n",
      " |      you will need to explicitly call `add_index_struct` on the `index_store`\n",
      " |      to update the index store.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |          index.index_struct.index_id = index_id\n",
      " |          index.storage_context.index_store.add_index_struct(index.index_struct)\n",
      " |      \n",
      " |      Args:\n",
      " |          index_id (str): Index id to set.\n",
      " |  \n",
      " |  update(self, document: llama_index.schema.Document, **update_kwargs: Any) -> None\n",
      " |      Update a document and it's corresponding nodes.\n",
      " |      \n",
      " |      This is equivalent to deleting the document and then inserting it again.\n",
      " |      \n",
      " |      Args:\n",
      " |          document (Union[BaseDocument, BaseIndex]): document to update\n",
      " |          insert_kwargs (Dict): kwargs to pass to insert\n",
      " |          delete_kwargs (Dict): kwargs to pass to delete\n",
      " |  \n",
      " |  update_ref_doc(self, document: llama_index.schema.Document, **update_kwargs: Any) -> None\n",
      " |      Update a document and it's corresponding nodes.\n",
      " |      \n",
      " |      This is equivalent to deleting the document and then inserting it again.\n",
      " |      \n",
      " |      Args:\n",
      " |          document (Union[BaseDocument, BaseIndex]): document to update\n",
      " |          insert_kwargs (Dict): kwargs to pass to insert\n",
      " |          delete_kwargs (Dict): kwargs to pass to delete\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  from_documents(documents: Sequence[llama_index.schema.Document], storage_context: Optional[llama_index.storage.storage_context.StorageContext] = None, service_context: Optional[llama_index.indices.service_context.ServiceContext] = None, show_progress: bool = False, **kwargs: Any) -> ~IndexType from abc.ABCMeta\n",
      " |      Create index from documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (Optional[Sequence[BaseDocument]]): List of documents to\n",
      " |              build the index from.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  docstore\n",
      " |      Get the docstore corresponding to the index.\n",
      " |  \n",
      " |  index_id\n",
      " |      Get the index struct.\n",
      " |  \n",
      " |  index_struct\n",
      " |      Get the index struct.\n",
      " |  \n",
      " |  service_context\n",
      " |  \n",
      " |  storage_context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.indices.base.BaseIndex:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  summary\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa45f0-3d05-4322-8ecf-ca560632ce24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "{{{,}}}"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "py3.10.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
